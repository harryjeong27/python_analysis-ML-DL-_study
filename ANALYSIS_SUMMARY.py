# [ 데이터 분석 ]
# - 데이터로부터 의미 있는 정보를 추출, 정보를 통해 미래를 예측하는 모든 행위
# ex)
# - 주가, 날씨, 수요 예측
# - 양/악성 유무 예측
# - 고객 이탈 예측
# - 연관성 분석 (장바구니 분석)
# - 고객의 세분화

# [ 데이터 분석의 분류 ]
# - 데이터마이닝 : 예측이 아닌 그래프나 수치분석 등을 통한 분석

# 인공지능 > 머신러닝 > 딥러닝
# - 머신러닝(기계학습)
# 1. 지도 학습 : Y값(target)을 알고 있는 경우의 분석
# 1) 회귀 분석 : Y가 연속형
# 2) 분류 분석 : Y가 이산형(범주형)
# - 트리기반 모델
# - 거리기반 모델
# - 확률/통계 모델
# - 신경망 모델 (딥러닝)

# 2. 비지도 학습 : Y값(target)을 알고 있지 않은 경우의 분석
# 1) 군집 분석 : 데이터 축소 테크닉 (세분류)
# 2) 연관성 분석 : 장바구니 분석
# 3) 딥러닝 (비정형데이터)

# ex) 목적 : 종양의 양/악성 유무 판별 예측
# f(x) = y
# y : 양성, 악성
# x : 종양의 크기, 모양, 색깔, ...
# --------------------------------------------------------------------------- #

# [ 파이썬에서의 데이터 분석 환경 ]
# 1) numpy : 숫자 데이터의 빠른 연산 처리
# 2) pandas : 정형 데이터의 입력
# 3) scikit-learn : 머신러닝 데이터 분석을 위한 모듈 (샘플 데이터 제공, 알고리즘)
#                   anaconda에 포함된 모듈
# 4) scipy : 조금 더 복잡한 과학적 연산 처리 가능 (선형 대수 ...)
#            anaconda에 포함된 모듈
# 5) matplotlib : 시각화 (anaconda에 포함된 모듈)
# 6) mglearn : 외부 모듈, 분석에서의 북잡한 시각화 함수 제공
# c:\Users\harryjeong> pip install mglearn
# c:\Users\harryjeong> ipython

# [ Data Analysis Process - 분류분석, 비통계적]
# 0) Setting Purpose of Analysis
# 1) data loading (데이터 수집 - Y의 분류에 영향을 미칠 것 같은 X들을 수집)
# 2) preprocessing (데이터 전처리 - 이상치/결측치 제거 및 수정)
# 3) model selection based on data (모델 선택)
# 4) data split into train/test (데이터 분리)
# 5) model fitting by train data set (모델 학습)
# 6) model scoring by test data set (모델 평가)
# - 비통계적이기 때문에 모델평가가 어려움 => 몇개 중 몇개를 맞췄는지로 판단
# - 모델의 학습과 평가의 데이터셋은 분리시킴 => 같은 데이터 사용 시 확률 높아지는 문제발생
# 7) model tuning (모델 튜닝)
# 7-1) parameter tuning => 매개변수 튜닝
# 7-2) feature importance check => 특성 중요도 튜닝 (설명변수 중요도)
# 8) review & apply
# --------------------------------------------------------------------------- #

# [ 분류분석 ]
# - 지도학습의 일부 (Y가 존재)
# - Y가 범주형인 경우
# - 거리기반 모델 (knn)
# - 트리기반 모델 (DT, RF, GB, XGB, ..)
# - SVM

# 1. knn (연속형에 적합)
# - 분류 모델 (지도학습)
# - 거리기반 모델
# - input data와 가장 거리가 가까운 k개의 관측치를 통해 input data의 Y값 결정
# - k개의 이웃이 갖는 정답(Y)의 평균 혹은 다수결로 최종 결론 내림
# - Y class의 개수의 배수가 되는 k수는 적절치 않음
# - 이상치에 민감한 모델 => 반드시 제거/수정
# - 스케일링에 매우 민감 (변수의 스케일 표준화)
# - 범주형 설명 변수가 많이 포함될수록 예측력 떨어짐 (거리화 시키기 어려움)
# - 학습되는 설명변수의 조합에 매우 민감
# - 고차원 데이터에 비적합
# - 내부 feature selection 기능 없음
# - 훈련 데이터 셋이 많아질수록 예측이 느린 경향 (대신 학습과정 생략, 데이터 들어올때마다 계산)
# - 설명변수가 많으면 가중치가 있지 않은 이상 좋지 않음

# 2. 트리기반 모델
# [ 트리기반 모델 ]
DT -> RF -> GB -> XGB -> ... 
# GB => 이전 트리 보완해서 다음 모델 만드는 구조, 오분류된 데이터를 정분류하도록 함 (TBC)
# 이런 발전 과정에도 대기업들은 RF, GB 많이 사용 - scikit learn에 포함

# 2.1 Decision Tree (트리기반 모델)
# - 분류 분석을 수행하는 트리기반 모델의 가장 시초 모델
# - 패턴 학습이 단순하여 패턴을 시각화 할 수 있음
# - 패턴이 Tree 구조를 띔
# - 비통계적 모델이므로 모델 해석이 매우 용이
# - 단일 의사결정이므로 예측력이 불안하거나 과대적합 가능성 있음

# [ 트리 기반 모델의 변수 선택 기능 ]
# - 트리 모델은 각 변수의 중요도를 자식 노드의 불순도를 통해 계산
# - 중요도가 높은 변수를 상위 split 조건으로 사용
# - 변수간 중요도 차이가 큰 경우 특정 변수만 재사용 가능성 있음
# - 트리 모델의 변수 중요도는 분석 시 중요 변수 파악에 용이하게 사용

# [ 불순도 ]
# - 특정 조건으로 인해 분리된 자식노드의 클래스의 혼합 정도
# - 주로 지니계수로 측정
# - 2개의 class를 갖는경우 f(p) = p(1-p)로 계산

# 예제) 
# - AAAAAA : p(A가 속할 확률) = 1, f(p) = 1 * (1-1) = 0
# - AAAABB : p = 4/6, f(p) = 4/6 * 2/6 = 0.22
# - AAABBB : p = 1/2, f(p) = 1/2 * 1/2 = 0.25

# decision tree에서 변수 중요도 확인
iris_dt2$variable.importance

# Petal.Width Petal.Length Sepal.Length  Sepal.Width 
# 62.64908     61.22238     39.67210     26.34410 

# 2.2 조건부 추론 나무
# - 기존 decision tree를 보안
# - 분류 기준에 따른 적합성을 통계적으로 해석하는 장치 추가
#   (변수의 유의성 검정)

# 2.3 Ramdom Forest
# - Decision Tree의 과대적합 현상을 해결하기 위해 여러개의 서로 다른
#   모양의 tree를 구성, 종합하여 최종 결론을 내는 방식
# - 각 트리가 독립적, 병렬처리에 유리(cpu당 작업 나눠줌)
# - Random Forest Classifier와 Random Forest Regressor 존재
# - 분류모델인 경우는 다수결로, 회귀모델인 경우는 평균으로 최종결론

# 2.4 Gradiant Boosting Tree(GB)
# - 이전 트리의 오차를 보완하는 트리를 생성하는 구조
# - 비교적 단순한 초기 트리를 형성, 오분류 data point에 더 높은 가중치를 부여, 오분류 data point
#   를 정분류 하도록 더 보완된, 복잡한 트리를 생성
# - learning rate 만큼의 오차 보완률 결정 (0 ~ 1, 높을수록 과적합 발생 가능성 높음)
# - random forest 모델보다 더 적은 수의 tree로도 높은 예측력을 기대할 수 있음
# - 각 트리는 서로 독립적이지 않으므로(이전 트리가 끝나야 다음 트리 시작)
#   병렬처리에 대한 효과를 크게 기대하기 어려움

# 2.5 SVM (Support Vector Machine)
# - 분류 기준을 회귀분석처럼 선형선, 혹은 초평면(다차원)을 통해 찾는 과정
# - 다차원(고차원) 데이터셋에 주로 적용
# - 초평면을 만드는 과정이 매우 복잡, 해석 불가 (black box 모델)
# - c(비선형성 강화), gamma(고차원선 강화)의 매개변수의 조합이 매우 중요 (상호 연관적)
# - 계수를 추정하는 방식이 회귀와 유사, 학습 전 변수의 scaling 필요
# - 이상치에 민감
# - 초기 분류기준으로부터 support vector에 가중치를 부여, 분류기준 강화 하는 과정
# - 지나치게 train 데이터에 맞추면 안됨 -> overfit
# - 과거에는 자주 쓰였으나 요즘에는 NN가 대부분을 대체함
# - 지지벡터 : 초평면을 그렸을 때 오분류된 데이터들 -> 얘네 기준으로 선을 확실히 확인 가능
# --------------------------------------------------------------------------- #

# [ 분석 시 고려 사항 ]
# 1. 변수 선택
# 2. 변수 변형 (결합 포함)
# 3. 교호작용 (interaction)
# 4. 교차검증 (cross validation)
# 5. 최적의 매개변수 조합 (grid search) : train/val/test
# 6. 변수 표준화 (scaling)

# [ 분석 시 고려사항 2. 변수 변형 (결합 포함) ]
# 2.1 종속 변수의 변형(NN 기반 모델일 경우 필수)
# - 종속변수가 범주형일때 하나의 종속변수를 여러 개의 종속변수로 분리시키는 작업
# - 모델에 따라 문자형태의 변수의 학습이 불가할 경우 종속변수를 숫자로 변경
# - NN에서는 주로 종속변수의 class의 수에 맞게 종속변수를 분리
# - 0과 1의 숫자로만 종속변수를 표현

# ex)
# Y   Y_남  Y_여
# 남    1     0
# 여    0     1
# 여    0     1 

df1 = DataFrame({'col1':['M','M','F','F'],
                'col2':[98,90,96,95]})

pd.get_dummies(df1)                           # 숫자 변수는 분할 대상 X
pd.get_dummies(df1, columns=['col1','col2'])  # 숫자 변수 강제 분할
pd.get_dummies(df1, drop_first=True)          # Y의 개수가 class -1개 분리

# Y  Y_A  Y_B  Y_C
# A   1    0    0
# B   0    1    0
# C   0    0    1
    
# Y  Y_A  Y_B  
# A   1    0   
# B   0    1    
# C   0    0     

# 2.2 변수 선택(feature selection)
# - 모델 학습 전 변수를 선택하는 과정
# - 트리기반, 회귀 모델 자체가 변수 선택하는 기준을 제시하기도 함
# - 거리기반, 회귀기반 모델들은 학습되는 변수에 따라 결과가 달라지므로
#   사전에 최적의 변수의 조합을 찾는 과정이 중요
# - 트리기반, NN기반 모델들은 내부 변수를 선택하는 과정이 포함,
#   다른 모델들에 비해 사전 변수 선택의 중요도가 낮음
  
# 2.2.1 모델 기반 변수 선택
# - 트리, 회귀 기반 모델에서의 변수 중요도를 사용하여 변수를 선택하는 방식
# - 트리에서는 변수 중요도를 참고, 회귀에서는 각 변수의 계수 참고
# - 모델에 학습된 변수끼리의 상관 관계도 함께 고려(종합적 판단)

# 2.2.2 변수선택 방법 2 : 일변량 통계 기법
# - 변수 하나와 종속변수와의 상관 관계 중심으로 변수 선택
# - 다른 변수가 함께 학습될때의 판단과는 다른 결과가 나올 수 있음
# - 학습 시킬 모델이 필요 없어 연산속도가 매우 빠름

# 2.2.3 변수선택 방법 3 : 반복적 선택(RFE)
# - step wise 기법과 유사
# - 전체 변수를 학습 시킨 후 가장 의미 없는 변수 제거,
#   반복하다 다시 변수 추가가 필요한 경우 추가하는 과정
# - 특성의 중요도를 파악하기 위한 모델 필요

# [ 분석 시 고려사항 3. 교호작용 ]
# 1차원 데이터셋으로는 예측력이 너무 적음(13%) -> 교호작용으로 예측력 높여보자 -> 각 차원의 확률 13%, 13%, 78%
# 좋은 방법은 아님 -> 가장 좋은 방법은 중요 변수가 뭔지 파악해서 집어 넣는 것
# 마땅히 떠오르는 변수가 없을 경우만 사용해보기
13(1차원) + 13(2차원) + 78(상호작용)

# [ 참고 : 파이썬 combination 출력 (발생 가능한 조합) ]
import itertools
list(itertools.combinations(['x1', 'x2', 'x3'], 2))    # choose(10, 2) in R

# 교호작용(interaction) 데이터 셋 => 혹시 변수 간에 의미 있는 관계가 있는지 체크
x1, x2, x2
- 2차원 교호작용 추가 : x1, x2, x3, x1x2, x1x3, x2x3, x1^2, x2^2, x3^2
- 3차원 교호작용 추가 : x1, x2, x3, x1x2, x1x3, x2x3, x1^2, x2^2, x3^2,
                    x1x2x3, x1^3, x2^3, x3^3
                    
# 주택가격 <- x1(지하 주차장 공급면적) * x2(강수량) => 강수량이라는 다른 변수를 넣으면 예측력이 확 높아질 수 있음 

# [ 분석 시 고려사항 5. 최적의 매개변수 조합 (grid search) : train/val/test ]
# - 변수의 최적의 조합을 찾는 과정
# - 중첩 for문으로 구현 가능, grid search 기법으로 간단히 구현 가능
# - train/validation/test set으로 분리
# - 매개변수의 선택은 validation set으로 평가
# - Grid는 for문 돌리지 않고도 알아서 진행해주는 것은 장점이지만, fit시킨 순간 매개변수가 고정되서
#   test data set scoring 할 때 매개변수 변화 불가 (이미 세팅된 매개변수로만 가능)
#   -> train data set과 비교 불가하여 overfit 확인 불가

# [ 분석 시 고려사항 6. 스케일링 ]
# - 설명변수의 서로 다른 범위를 동일한 범주내 비교하기 위한 작업
# - 거리기반 모델, 회귀계수의 크기 비교, NN의 모델 등에서 필요로 하는 작업
# - 각 설명변수의 중요도를 정확히 비교하기 위해서도 요구되어짐
# - 특히 interaction 고려 시 결합되는 설명변수끼리의 범위를 동일하게 만들 필요 있음

# --------------------------------------------------------------------------- #

# [ 분류분석 활용 1 : PCA + knn ]
# 1. PCA(Principal Component Analysis : 주성분 분석)
# - 비지도 학습
# - 기존의 변수로 새로운 인공변수를 유도하는 방식 (변수 결합)
# - 유도된 인공변수끼리 서로 독립적
# - 첫번째 유도된 인공변수가 기존 데이터의 분산을 가장 많이 설명하는 형식
# - 회귀의 다중공선성 문제 해결
# - 기존 데이터를 모두 사용, 저차원 모델 생성 가능 => 과대적합 해소
# - 의미 있는 인공변수 유도 => 설명변수를 추가할수록 좋음
# - 변수 scaling 필요

# Y = X1 + X2 + X3 + X4 + X5               # 다중공선성이 예상됨 => 인공변수를 넣자
# C1 = a1X1 + a2X2 + a3X3 + a4X4 + a5X5    # 가장 설명력이 높음
# C2 = b1X1 + b2X2 + b3X3 + b4X4 + b5X5    # 그다음 설명력이 높음

# Y = c1C1 + c2C2    # PCA + regressor 

# 2. 이미지 인식 / 분석 : PCA + knn
# sklearn에서의 이미지 데이터 셋
# - 2000년 초반 이후 유명인사 얼굴 데이터
# - 전처리 속도를 위해 흑백으로 제공
# - 총 62명의 사람의 얼굴을 여러 장 촬영한 데이터 제공
# - 총 3023개의 이미지 데이터 (행의 수), 87x65 (5655) 픽셀로 규격화 제공 (컬럼 수)